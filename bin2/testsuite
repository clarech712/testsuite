#!/usr/bin/env bash
#
# testsuite
# Run test profiles through testapp to determine cluster status
#
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
HOME_LOCAL="$(realpath "$SCRIPT_DIR/../..")"

# Arrays to hold results of current tests
# Each test has a single (zero-based) index in these arrays
#
TESTNAMES=()
TEST_SLURMARGS=()
TEST_TESTAPPARGS=()
TESTTYPES=()
JOBCOUNTS=()
PASSCOUNTS=()
PASSES=()
FAILS=()
TESTSTATUS=()
TESTPERCENTAGE=()

# Arrays to hold data for each job
# Each job has a single (zero-based) index in these arrays
# The index is computed based on a exclusive prefix sum of the JOBCOUNTS array
# with a additional offset based on the job number within the test
# i.e. The 4th job in test 4 has index: JOBCOUNTS[0] + JOBCOUNTS[1] + JOBCOUNTS[2] + 3
#
# I blame this mess on bash not supporting multi-dimensional arrays
#
# Statuses are PASSED=0, FAILED=1, RUNNING=2, NOT LAUNCHED=3
#
JOBSTATUS=()
PIDS=()

# Store an exclusive prefix sum of the JOBCOUNT array to make indexing JOBSTATUS and PID easier
#
BASEINDEX=()

# Variables to hold status of tests and jobs in progress
#
TOTALTESTS=0
TOTAL_SERIAL_TESTS=0
TOTAL_CONCURRENT_TESTS=0
TOTALJOBS=0
FINISHEDTESTS=0
FINISHEDJOBS=0
PASSEDTESTS=0
FAILEDTESTS=0
SYSLOG=0

# Print generic usage statement
#
function usage {
	echo "Usage:  testsuite [options] [test profiles] [-o SLURM options]
Options:
         -l          list available tests
         -n          output any nodes which fail tests
         -k          keep testsuite/testapp output after tests have finished
                     (useful for seeing why they failed if working a problem)
         -t <path>   use <path> for temporary output directories (such as -k)
         -v          verbose output (default)
         -q          quiet (no verbose output)
         -r          force all tests to be run on random nodes
         -R          force all test to NOT run on random nodes
         -o          options to pass directly to SLURM (not testapp)
                      (this option must come last, after the profile names)
" 1>&2
}

THISHOST=`hostname -s`
if [[ $THISHOST == *"-adm" ]]; then
	usage
	echo -e "\nPlease do not run this on an admin server. They get sad.\n"
	exit 117
fi

# Make sure we don't leave a mess once we're finished
#
function cleanupoutput {
	if [ -d ${1} ]; then
		# Attempting this may get hung up on NFS issues, so dance around them.
		#
		rm -rf "${1}" 2> /dev/null
		if [ ${VERBOSE} -eq 1 ]; then
			echo -n "Cleaning up ${2} working directory."
		fi
		NFSLOCKS=(${1}/.nfs*)
		while [ ${#NFSLOCKS[@]} -ne 1 -o -e ${NFSLOCKS[0]} ]; do
			if [ ${VERBOSE} -eq 1 ]; then
				echo -n "."
			fi
			sleep 2
			NFSLOCKS=(${1}/.nfs*)
		done
		if [ ${VERBOSE} -eq 1 ]; then
			echo "done."
		fi
		rm -rf "${1}"
	fi
}

# Remove the jobs that we started
#
function cleanupjobs {

	# Go ahead and kill everything that is left
	#
	COUNTKILLED=0

	for I in `seq 1 ${TOTALTESTS}`; do
		TESTINDEX=`expr ${I} - 1`
		for J in `seq 1 ${JOBCOUNTS[$TESTINDEX]}`; do

			JOBNO=`expr ${J} - 1`
			JOBINDEX=`expr ${BASEINDEX[$TESTINDEX]} + ${JOBNO}`
			PID=${PIDS[$JOBINDEX]}

			# Check if the job is still running and kill it if so
			#
			if [ ${JOBSTATUS[$JOBINDEX]} -eq 2 ]; then
				# 2> /dev/null is in case a job has finished before we get here
				#
				if [ ${VERBOSE} -eq 1 ]; then
					if [ ${COUNTKILLED} -eq 0 ]; then
						echo "Killing unfinished testapp instances..."
						COUNTKILLED=`expr ${COUNTKILLED} + 1`
					fi
					echo "Killing testapp with PID ${PID}"
				fi

				if [ ${PID} -ne 0 ]; then
					kill ${PID} 2> /dev/null
				fi

				# Cleanup after testapp.
				#
				HOSTNAME=`hostname | sed 's/\..*$//'`
				cleanupoutput "${TMP}/testapp_${HOSTNAME}_${PID}" "testapp PID ${PID}"
			fi
		done
	done
}

# Launch the jobs after they have been parsed
# For serial tests, only a single job is launched
# For concurrent tests, every job is launched
#
function launchjobs {
	if [ ${VERBOSE} -eq 1 ]; then
		echo "Launching tests...."
	fi

	for I in `seq 1 ${TOTALTESTS}`; do

		# 0 based indexing
		TESTINDEX=`expr $I - 1`

		TESTNAME=${TESTNAMES[$TESTINDEX]}
		SLURMARGS=${TEST_SLURMARGS[$TESTINDEX]}
		TESTAPPARGS=${TEST_TESTAPPARGS[$TESTINDEX]}
		JOBCOUNT=${JOBCOUNTS[$TESTINDEX]}

		JOBINDEX=0

		if [ ${VERBOSE} -eq 1 ]; then
			echo -n "Launching test: ${TESTNAME} with"
			if [ "x`echo ${SLURMARGS} | sed 's/ //g'`" == "x" ]; then
				echo -n "out SLURM options and with"
			else
				echo -n " SLURM options: `echo ${SLURMARGS} | sed 's/^[ \t]*\(.*\)[ \t]*$/\1/g'` and with"
			fi
			if [ "x`echo ${TESTAPPARGS} | sed 's/ //g'`" == "x" ]; then
				echo -n "out testapp options"
			else
				echo -n " testapp options: `echo ${TESTAPPARGS} | sed 's/^[ \t]*\(.*\)[ \t]*$/\1/g'`"
			fi
			echo ""
		fi

		# Tests that must run serially (one at a time) require testapp to run
		# with -v so they can extract the job IDs needed to set dependencies.
		#
		if [ "x${TESTTYPES[$TESTINDEX]}" == "xS" ]; then
			TESTAPPARGS="${TESTAPPARGS} -v"
		fi
		DEPEND=""

		for J in `seq 1 ${JOBCOUNTS[$TESTINDEX]}`; do
			J=`expr ${J} - 1`
			JOBINDEX=`expr ${BASEINDEX[$TESTINDEX]} + $J`

			# Spin up background jobs
			# Output filenames have name format <test_id NO>_<test_name>_<job NO>_<jobcount>
			# Each test will usually result in multiple job launches
			#
			NEWIND=`expr $J + 1`
			touch "${TMP}/${TESTINDEX}_${TESTNAME}_${NEWIND}_${JOBCOUNT}"

			# So this is what this does:
			# Launch the test in the background,
			# Do it in a subshell so job management output doesn't intermingle with normal output,
			# Redirect test output into a specific file,
			# Then print and save the PID of the testapp instance
			#
			sleep ${STAGGERDELAY}
			PIDS[$JOBINDEX]=`(bash ${SCRIPT_DIR}/testapp ${TESTAPPARGS} -t ${TMP} ${TESTNAME} -o ${SLURMARGS} ${DEPEND} >> "${TMP}/${TESTINDEX}_${TESTNAME}_${NEWIND}_${JOBCOUNT}" 2>&1 & echo $!)`
			echo "Spawned testapp PID ${PIDS[$JOBINDEX]}." >> "${TMP}/${TESTINDEX}_${TESTNAME}_${NEWIND}_${JOBCOUNT}"
			JOBSTATUS[$JOBINDEX]=2
			JOBS_RUN[$TESTINDEX]=${JOBCOUNT}

			# For tests that must run serially (one at a time),
			# set job dependencies to ensure that happens.
			#
			if [ "x${TESTTYPES[$TESTINDEX]}" == "xS" ]; then
				JOBID=""
				while [ "x${JOBID}" == "x" ]; do
					sleep 1
					JOBID=`grep '^Started job ' "${TMP}/${TESTINDEX}_${TESTNAME}_${NEWIND}_${JOBCOUNT}" 2> /dev/null | cut -d ' ' -f 3`
				done
				if [ "x${DEPEND}" == "x" ]; then
					DEPEND="--dependency=after"
				fi
				DEPEND="${DEPEND}:${JOBID}"
			fi
		done
	done
	if [ ${VERBOSE} -eq 1 ]; then
		echo ""
	fi
}

# The basic idea is that we launch a large group of testapp commands in the background
# They then dump their output into files in our temp directory
# We then parse those files to determine how many tests did and did not fail
#
function scandirectory {
	# Reset the stats
	#
	FINISHEDTESTS=0
	FINISHEDJOBS=0
	PASSEDTESTS=0
	FAILEDTESTS=0

	# Scan through everything in the directory
	#
	for TESTINDEX in `seq 1 ${TOTALTESTS}`; do

		# Account for 0 based indexing
		#
		TESTINDEX=`expr ${TESTINDEX} - 1`

		# Get test details
		#
		TESTNAME=${TESTNAMES[$TESTINDEX]}
		SLURMARGS=${TEST_SLURMARGS[$TESTINDEX]}
		TESTAPPARGS=${TEST_TESTAPPARGS[$TESTINDEX]}
		JOBCOUNT=${JOBCOUNTS[$TESTINDEX]}

		# Reset to zero (or 2) so we get the correct count
		#
		PASSES[$TESTINDEX]=0
		FAILS[$TESTINDEX]=0
		TESTSTATUS[$TESTINDEX]=2
		TESTPERCENTAGE[$TESTINDEX]=""

		PERCENTAGE=""

		# loop over each job so we can check which jobs have and have not finished
		# use BASEINDEX[$TESTINDEX] + JOBNO - 1 to find this job's index in the flat array
		#
		for JOBNO in `seq 1 ${JOBCOUNTS[$TESTINDEX]}`; do

			JOBINDEX=`expr ${BASEINDEX[$TESTINDEX]} + ${JOBNO} - 1`
			PREV=`expr $JOBINDEX - 1`

			JOBOUTPUTFILE="${TMP}/${TESTINDEX}_${TESTNAME}_${JOBNO}_${JOBCOUNT}"

			# We need to grep for a pass or failure, and also check if a performance based test was run
			#
			if [ -f "${JOBOUTPUTFILE}" ] && [ `grep -c -h "^PASS" ${JOBOUTPUTFILE}` -eq 1 ]; then
				PASSES[$TESTINDEX]=`expr ${PASSES[$TESTINDEX]} + 1`
				PERCENTAGE=`grep -h -m 1 "^PASS" ${JOBOUTPUTFILE} | cut -s -d '%' -f 1 | rev | cut -s -d ' ' -f 1 | rev`
				if [ -n "${PERCENTAGE}" ]; then
					TESTPERCENTAGE[$TESTINDEX]="${TESTPERCENTAGE[$TESTINDEX]}, ${PERCENTAGE}%"
				fi
				JOBSTATUS[$JOBINDEX]=0

			elif [ -f "${JOBOUTPUTFILE}" ] && [ `grep -c -h "^\(FAIL\)\|\(testapp:  Error! \)" ${JOBOUTPUTFILE}` -ge 1 ]; then
				FAILS[$TESTINDEX]=`expr ${FAILS[$TESTINDEX]} + 1`
				PERCENTAGE=`grep -h -m 1 "^FAIL" ${JOBOUTPUTFILE} | cut -s -d '%' -f 1 | rev | cut -s -d ' ' -f 1 | rev`
				if [ -n "${PERCENTAGE}" ]; then
					TESTPERCENTAGE[$TESTINDEX]="${TESTPERCENTAGE[$TESTINDEX]}, ${PERCENTAGE}%"
				fi
				JOBSTATUS[$JOBINDEX]=1
			fi

			PREV=`expr ${JOBINDEX} - 1`
		done

		FINISHEDJOBS=`expr ${FINISHEDJOBS} + ${PASSES[$TESTINDEX]} + ${FAILS[$TESTINDEX]}`

		# Check for test success or failure
		#
		if [ ${FAILS[$TESTINDEX]} -gt `expr ${JOBCOUNTS[$TESTINDEX]} - ${PASSCOUNTS[$TESTINDEX]}` ]; then
			FAILEDTESTS=`expr ${FAILEDTESTS} + 1`
			TESTSTATUS[$TESTINDEX]=1
			FINISHEDTESTS=`expr ${FINISHEDTESTS} + 1`
		elif [ ${PASSES[$TESTINDEX]} -ge ${PASSCOUNTS[$TESTINDEX]} ]; then
			PASSEDTESTS=`expr ${PASSEDTESTS} + 1`
			TESTSTATUS[$TESTINDEX]=0
			FINISHEDTESTS=`expr ${FINISHEDTESTS} + 1`
		fi
	done
}

# Print out the latest results as determined by scandirectory
#
function printstats {
	printf "Status  Test                       Pass   Fail    Run  Total\n"
	for I in `seq 1 ${TOTALTESTS}`; do
		TESTINDEX=`expr ${I} - 1` # Zero based indexing
		NAME=${TESTNAMES[$TESTINDEX]}
		PASS=${PASSES[$TESTINDEX]}
		FAIL=${FAILS[$TESTINDEX]}
		COUNT=${JOBCOUNTS[$TESTINDEX]}
		RUNNING=`expr ${COUNT} - ${PASS} - ${FAIL}`
		DONE=`expr ${PASS} + ${FAIL}`
		RESULT=""
		# Check for test success or failure
		#
		if [ ${FAILS[$TESTINDEX]} -gt `expr ${COUNT} - ${PASSCOUNTS[$TESTINDEX]}` ]; then
			RESULT="FAIL   "
		elif [ ${PASSES[$TESTINDEX]} -ge ${PASSCOUNTS[$TESTINDEX]} ]; then
			RESULT="PASS   "
		else
			RESULT="RUNNING"
		fi

		printf "%s %-25s" "${RESULT}" "${NAME}"
		printf "%6d %6d %6d %6d\n"  "${PASS}" "${FAIL}" "${RUNNING}" "${COUNT}"
		if [ ${SYSLOG} -eq 1 ]; then
			logger "testsuite ${NAME} ${RESULT} ${PASS} ${FAIL} ${RUNNING} ${COUNT}"
		fi
		if [ -n "${TESTPERCENTAGE[$TESTINDEX]}" ]; then
			echo "        (`echo ${TESTPERCENTAGE[$TESTINDEX]} | cut -c 3-`)"
		fi
	done
	echo ""
}

# Echos out a list of nodes that failed on the test
#
function listfailednodes {
	FAILEDNODES=""

	# Scan through everything in the directory and print all the failed nodes
	#
	for TESTINDEX in `seq 1 ${TOTALTESTS}`; do
		TESTINDEX=`expr ${TESTINDEX} - 1`
		COUNT=${JOBCOUNTS[$TESTINDEX]}
		for INDEX in `seq 1 ${COUNT}`; do
			if [ -f "${TMP}/${TESTINDEX}_${TESTNAMES[$TESTINDEX]}_${INDEX}_${COUNT}" ]; then
				FAILEDNODES="${FAILEDNODES} `grep '^FAIL' ${TMP}/${TESTINDEX}_${TESTNAMES[$TESTINDEX]}_${INDEX}_${COUNT} | cut -d ' ' -f 4-`"
			fi
		done
	done

	echo ${FAILEDNODES} | sort -u
}

# Define variables for the delay between scans, before the first scan,
# between calls to start a test (so as to not overload sbatch), and the
# grace timer after enough jobs are done to already make the call before
# we just kill the rest.
#
LOOPDELAY=10
STARTSCANDELAY=10
STAGGERDELAY=0.5
GRACETIMER=300

# Use fixed directory for profiles
#
PROFILEROOT="${HOME_LOCAL}/testsuite/profiles"
PROFILEHOMEDIR=".testsuite/"

# By default, create a temporary working directory in the user's scratch.
#
TMPROOT="$HOME_LOCAL"

# Use extra output information
#
VERBOSE=1

# Don't cleanup temp directory (messy)
#
KEEPOUTPUT=0

# List nodes from failed tests
#
LISTNODES=0

# Force every test to be run on random nodes
#
FORCERANDOM=0

# Remove all random flags from the test profile
#
REMOVERANDOM=0

PROFILES=""
AVAILABLE_TESTS=(`./testapp -l`)

LISTPROFILES=0

SLURMOPTS=0
GLOBALSLURMARGS=""

MYTMP=0

while [ $SLURMOPTS -eq 0 ]; do

	# Check for verbose (-v) flag
	#
	if [ "x${1}" == "x-v" ]; then
		VERBOSE=1

	# Check for quiet (-q) flag
	#
	elif [ "x${1}" == "x-q" ]; then
		VERBOSE=0

	# Check for keep output (-k) flag
	#
	elif [ "x${1}" == "x-k" ]; then
		KEEPOUTPUT=1

	# Check for path for temporary output directories (-t path)
	#
	elif [ "x${1}" == "x-t" ]; then
		TMPROOT="${2}"
		MYTMP=1
		shift

	# Check for list (failed) nodes (-n) flag
	#
	elif [ "x${1}" == "x-n" ]; then
		LISTNODES=1

	# Check for list profiles (-l) flag
	#
	elif [ "x${1}" == "x-l" ]; then
		LISTPROFILES=1

	# Check for the remove random flag
	#
	elif [ "x${1}" == "x-R" ]; then
		REMOVERANDOM=1

	# Check for the force random flag
	#
	elif [ "x${1}" == "x-r" ]; then
		FORCERANDOM=1

	# If ANY of the arguments the help option, print usage and exit
	#
	elif [ "x${1}" == "x-h" -o "x${1}" == "x--help" ]; then
		usage
		exit 0

	# (-o) All arguments that follow (if any) are intended for SLURM [not testapp].
	#
	elif [ "x${1}" == "x-o" -o "x${1}" == "x" ]; then
		SLURMOPTS=1

	# We must have a test profile otherwise
	#
	else
		PROFILES="${PROFILES} ${1}"
	fi

	shift
done

if [[ -z ${CLUSTER_SCRATCH} && ${MYTMP} -eq 0 ]]; then
	echo "testsuite: Warning! \$CLUSTER_SCRATCH is undefined."
	echo "Will be creating temporary space in your home directory."

	# Try...
	TMPROOT=$HOME_LOCAL/testsuite_tmp
        echo $TMPROOT

	if [[ -e ${TMPROOT} ]]; then
		if [[ ! -d ${TMPROOT} ]]; then
			echo "Error! Tried to use ${TMPROOT} as working directory, but it already exists as a file."
			echo "Please specify another directory with the -t option."
			exit 5
		fi
	fi

	if [[ ! -d ${TMPROOT} ]]; then
		mkdir ${TMPROOT} 2> /dev/null
		if [ $? -ne 0 ]; then
		        echo "Error!  Failed to create temp directory '${TMPROOT}'." 1>&2
			exit 5
		else
			echo "Created temporary working directory '${TMPROOT}'."
		fi
	fi
fi

# Print usage statement if incompatible options are given.
#
if [ ${LISTPROFILES} -eq 1 -a \( ${KEEPOUTPUT} -eq 1 -o ${LISTNODES} -eq 1 \) ]; then
	usage
	exit 2
elif [ ${FORCERANDOM} -eq 1 -a ${REMOVERANDOM} -eq 1 ]; then
	usage
	exit 2
fi

# Verify the temporary output directory exists.
#
if [ "x${TMPROOT}" == "x" -o ! -d "${TMPROOT}" ]; then
	echo "testsuite:  Error!  Temporary output directory '${TMPROOT}' could not be found." 1>&2
	exit 3
fi

# Define variables for creating our temporary directory.
TMP="${TMPROOT}/testsuite_${HOSTNAME}_$$"
echo $TMP
TESTFILE="${TMP}/tests"

# Build and print list of available profiles if -l option was given.
#
if [ ${LISTPROFILES} -eq 1 ]; then
	for PROFILE in ${HOME_LOCAL}/${PROFILEHOMEDIR}/*; do
		PROFILENAME="$(basename ${PROFILE})"
		if [ "x${PROFILENAME}" != "x*" ]; then
			echo "${PROFILENAME}  (Personal Profile)"
		fi
	done
	for PROFILE in ${PROFILEROOT}/*; do
		PROFILENAME="$(basename ${PROFILE})"
		if [ "x${PROFILENAME}" != "x*" ] && [ "${PROFILENAME}" != "Archive" ] && [ "${PROFILENAME}" != "split.py" ]; then
			if [ -e "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILENAME}" ]; then
				echo "${PROFILENAME}  (Overridden)"
			else
				echo "${PROFILENAME}"
			fi
		fi
	done
	exit 0
fi

# If no test profiles were requested, run the "default" profile by default.
#
if [ ! -n "${PROFILES}" ]; then
	PROFILES="default"
	if [ ${VERBOSE} -eq 1 ]; then
		echo "No profiles named.  Running the 'default' profile."
	fi
fi

# Create trap handler
#
trap 'cleanupjobs; if [ ${KEEPOUTPUT} -eq 0 ]; then cleanupoutput ${TMP} "testsuite"; fi; trap 2; kill -2 $$' 2 15

# We'll pass these onward to SLURM through testapp with the -o flag
#
GLOBALSLURMARGS="${*}"

# Create temporary directory to hold results file during processing
#
if [ -d ${TMP} ]; then
	echo "testsuite:  Error!  Temp directory '${TMP}' already exists." 1>&2
	exit 4
fi
mkdir ${TMP} 2> /dev/null
if [ $? -ne 0 ]; then
	echo "testsuite:  Error!  Failed to create temp directory '${TMP}'." 1>&2
	exit 5
elif [ ${VERBOSE} -eq 1 ]; then
	echo "Created working directory '${TMP}'."
fi

# Move to tmp
#
cd ${TMP}

# Ensure there are no obvious syntax errors in one of the requested profiles.
#
for PROFILE in ${PROFILES}; do
	if [ -e "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" ]; then
		INVALIDLINES=$(cat "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" \
			| grep -v '^\(S\|C\) [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' \
                        | grep -v '^#')
		if [ -n "${INVALIDLINES}" ]; then
			echo "testsuite:  Error!  Invalid lines in profile '${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}':"
			echo "------------------------------------------------------------------------------"
			echo "${INVALIDLINES}"
			echo "------------------------------------------------------------------------------"
			cleanupoutput
			exit 7
		fi
	elif [ -e "${PROFILEROOT}/${PROFILE}" ]; then
		INVALIDLINES=$(cat "${PROFILEROOT}/${PROFILE}" \
			| grep -v '^\(S\|C\) [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' \
			| grep -v '^#')
		if [ -n "${INVALIDLINES}" ]; then
			echo "testsuite:  Error!  Invalid lines in profile '${PROFILEROOT}/${PROFILE}':"
			echo "------------------------------------------------------------------------------"
			echo "${INVALIDLINES}"
			echo "------------------------------------------------------------------------------"
			cleanupoutput
			exit 7
		fi
	fi
done

# Move all the tests into one file
# Check that we actually have profiles, else cat will hang on STDIN
# Do this in two passes so that serial tests get priority over concurrent tests.
# (Serial tests will take longer, so this should provide a faster total run time.)
#
touch "${TESTFILE}"
for PROFILE in ${PROFILES}; do
	if [ -e "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" ]; then
		cat "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" | grep '^S [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' >> "${TESTFILE}"
	elif [ -e "${PROFILEROOT}/${PROFILE}" ]; then
		cat "${PROFILEROOT}/${PROFILE}" | grep '^S [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' >> "${TESTFILE}"
	fi
done
for PROFILE in ${PROFILES}; do
	if [ -e "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" ]; then
		cat "${HOME_LOCAL}/${PROFILEHOMEDIR}/${PROFILE}" | grep '^C [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' >> "${TESTFILE}"
	elif [ -e "${PROFILEROOT}/${PROFILE}" ]; then
		cat "${PROFILEROOT}/${PROFILE}" | grep '^C [a-zA-Z_0-9-]\+ [1-9]\+[0-9]* [1-9]\+[0-9]*.*$' >> "${TESTFILE}"
	fi
done

# Check if our test file is good
#
if [ ! -s ${TESTFILE} ]; then
	echo "testsuite:  Error!  No valid profile(s) specified." 1>&2
	echo "" 1>&2
	usage
	cleanupoutput ${TMP} "testsuite" 1>&2
	exit 6
fi

# Test index value
# Each test will have a unique ID
#
I=0

#Parse tests first
#
while read testline; do
	# Parse each line according the format
	#
	TESTTYPE=$(echo "$testline" | cut -d ' ' -f 1)
	TESTNAME=$(echo "$testline" | cut -d ' ' -f 2)
	PASSCOUNT=$(echo "$testline" | cut -d ' ' -f 3)
	JOBCOUNT=$(echo "$testline" | cut -d ' ' -f 4)
	TESTAPPOPTS=$(echo "$testline" | cut -d ' ' -f 5-)

	# Sanity check
	#
	if [ ${PASSCOUNT} -gt ${JOBCOUNT} ]; then
		if [ ${VERBOSE} -eq 1 ]; then
			echo "Invalid pass count for test ${TESTNAME}"
		fi
		# Move to next test
		#
		continue
	fi

	# Check for test availability
	#
	for AVAILABLE_TEST in ${AVAILABLE_TESTS[@]}; do
		if [ "x${TESTNAME}" == "x${AVAILABLE_TEST}" ]; then
			# Save the test now that we know it is available
			#

			# Initialize the test information
			#
			TESTNAMES[$I]=${TESTNAME}
			PASSCOUNTS[$I]=${PASSCOUNT}
			JOBCOUNTS[$I]=${JOBCOUNT}
			PASSES[$I]=0
			FAILS[$I]=0
			TESTSTATUS[$I]=2
			TESTPERCENTAGES[$I]=""
			TESTTYPES[$I]=${TESTTYPE}

			# Create the prefix sum array for indexing job statuses
			#
			if [ $I -gt 0 ]; then
				PREV=$(expr $I - 1)
				BASEINDEX[$I]=$(expr ${BASEINDEX[$PREV]} + ${JOBCOUNTS[$PREV]})
			else
				BASEINDEX[$I]=0
			fi

			# Build testapp args
			# We have to do this because each test could specify both testapp args and SLURM args
			# The SLURM args must be passed to testapp after the -o flag,
			# while the testapp args must go before the testname
			#
			# However, do try to avoid using the test profile to pass arguments to SLURM
			# because GLOBALSLURMARGS could conflict with the per test SLURMARGS arguments
			#
			TESTAPPARGS=$(echo ${TESTAPPOPTS} | awk 'BEGIN{FS=" -o "}{print $1}')
			SLURMARGS=$(echo ${TESTAPPOPTS} | awk 'BEGIN{FS=" -o "}{print $2}')
			SLURMOPTIONS=""

			if [ ${REMOVERANDOM} -eq 1 ]; then
				TESTAPPARGS=$(echo ${TESTAPPARGS} | sed 's/-r//g')
			elif [ ${FORCERANDOM} -eq 1 ]; then
				TESTAPPARGS="${TESTAPPARGS} -r"
			fi

			if [ ${KEEPOUTPUT} -eq 1 ]; then
				TESTAPPARGS="${TESTAPPARGS} -k -v"
			fi

			if [ -n "${GLOBALSLURMARGS}" ]; then
				SLURMOPTIONS="${SLURMOPTIONS}${GLOBALSLURMARGS} "
			fi

			if [ -n "${SLURMARGS}" ]; then
				SLURMOPTIONS="${SLURMOPTIONS}${SLURMARGS} "
			fi

			# Save job arguments for later launch
			#
			TEST_TESTAPPARGS[$I]="${TESTAPPARGS}"
			TEST_SLURMARGS[$I]="${SLURMOPTIONS}"


			if [ "x{$TESTTYPE}" == "xC" ]; then
				TOTAL_CONCURRENT_TESTS=$(expr ${TOTAL_CONCURRENT_TESTS} + 1)
			else
				TOTAL_SERIAL_TESTS=$(expr ${TOTAL_SERIAL_TESTS} + 1)
			fi

			TOTALJOBS=$(expr ${TOTALJOBS} + ${JOBCOUNT})

			# Set the job statuses now
			#
			for J in $(seq 1 ${JOBCOUNT}); do
				J=$(expr $J - 1)
				JOBINDEX=$(expr ${BASEINDEX[$I]} + $J)

				# Set JOBSTATUS to Not Running
				#
				JOBSTATUS[$JOBINDEX]=3

				# Set PID to 0, we'll check this later if we have to kill any unfinished jobs
				#
				PIDS[$JOBINDEX]=0
			done

			# Increment test ID
			#
			I=$(expr ${I} + 1)
			break
		fi
	done
done < ${TESTFILE}

TOTALTESTS=$(expr ${TOTAL_SERIAL_TESTS} + ${TOTAL_CONCURRENT_TESTS})

if [ ${TOTALTESTS} -ne 0 ]; then

	# Launch the jobs
	#
	launchjobs

	# Keep searching through the directory until everything has passed or failed, (or someone signals us)
	#
	sleep ${STARTSCANDELAY}
	while [ ${FINISHEDTESTS} -ne ${TOTALTESTS} ]; do
		scandirectory
		if [ ${VERBOSE} -eq 1 ]; then
			printstats
		fi
		sleep ${LOOPDELAY}
	done

	# Keep running the grace timer or quit once all the jobs are done
	#
	while [ ${FINISHEDJOBS} -ne ${TOTALJOBS} -a ${GRACETIMER} -gt 0 ]; do
		scandirectory
		if [ ${VERBOSE} -eq 1 ]; then
			printstats
		fi
		sleep ${LOOPDELAY}
		GRACETIMER=$(expr ${GRACETIMER} - ${LOOPDELAY})
	done

	# Cleanup remaining jobs
	#
	cleanupjobs

	# Print final report
	#
	SYSLOG=1
	printstats
else
	echo "No tests launched"
fi

if [ ${LISTNODES} -eq 1 ]; then
	listfailednodes 1>&2
fi

if [ ${KEEPOUTPUT} -eq 0 ]; then
	cleanupoutput ${TMP} "testsuite"
elif [ ${VERBOSE} -eq 1 ]; then
	echo "Leaving working directory.  You will need to manually remove this by doing:"
	echo "    rm -rf ${TMP}"
fi

if [ ${FAILEDTESTS} -ne 0 ]; then
	exit 1
fi

exit 0
